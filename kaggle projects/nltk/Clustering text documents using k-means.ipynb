{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c8deec-6155-4e52-8e43-19c2f28d171c",
   "metadata": {},
   "source": [
    "Clustering text documents using k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bfa1789-39f3-4575-b74b-b10fdefc3309",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_20newsgroups\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize \u001b[38;5;66;03m#Used to extract words from documents\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer \u001b[38;5;66;03m#Used to lemmatize words\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import word_tokenize #Used to extract words from documents\n",
    "from nltk.stem import WordNetLemmatizer #Used to lemmatize words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae73624-9878-4aeb-a1a0-33ae78858c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Selected 3 categories from the 20 newsgroups dataset\n",
    "\n",
    "categories = [\n",
    "    'talk.religion.misc',\n",
    "    'comp.graphics',\n",
    "    'sci.space',\n",
    "]\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "import nltk\n",
    "import sklearn\n",
    "print('The nltk version is {}.'.format(nltk.__version__))\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e292861f-d0be-4018-b633-38dd990be2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = fetch_20newsgroups(subset='all', categories=categories, \n",
    "                             shuffle=False, remove=('headers', 'footers', 'quotes'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547ba60-8116-45b2-bcf6-de46e20a4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.target\n",
    "true_k = len(np.unique(labels)) ## This should be 3 in this example\n",
    "print(true_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400d673-cfbc-45b6-b456-76c1670d0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for i in range(len(df.data)):\n",
    "    word_list = word_tokenize(df.data[i])\n",
    "    lemmatized_doc = \"\"\n",
    "    for word in word_list:\n",
    "        lemmatized_doc = lemmatized_doc + \" \" + lemmatizer.lemmatize(word)\n",
    "    df.data[i] = lemmatized_doc  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0505d56b-7a1d-4795-a1de-7fd0998374a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c749b4-f3dc-4ac4-8a41-7ef2918f6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english', min_df=2) ## Corpus is in English\n",
    "X = vectorizer.fit_transform(df.data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfc8f6d-7ff5-475d-a639-57e70d847c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62170b1a-a1ed-455a-a5bf-52b156364ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9377ce-b67f-444c-942b-c51d3667a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83a29f2-f250-4680-8de6-578f0e3fdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "centroids = km.cluster_centers_.argsort()[:, ::-1] ## Indices of largest centroids' entries in descending order\n",
    "terms = vectorizer.get_feature_names_out() \n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367fa03b-d169-45ab-8a14-c032a273c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b308beaf-8c13-460f-964c-46318419d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def frequencies_dict(cluster_index):\n",
    "    if cluster_index > true_k - 1:\n",
    "        return\n",
    "    term_frequencies = km.cluster_centers_[cluster_index]\n",
    "    sorted_terms = centroids[cluster_index]\n",
    "    frequencies = {terms[i]: term_frequencies[i] for i in sorted_terms}\n",
    "    return frequencies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2554c8ac-324f-4f83-a763-2d8dfd463400",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def makeImage(frequencies):\n",
    "\n",
    "    wc = WordCloud(background_color=\"white\", max_words=50)\n",
    "    # generate word cloud\n",
    "    wc.generate_from_frequencies(frequencies)\n",
    "\n",
    "    # show\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cc6b3f-1052-4b0c-b25e-9b736d28899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(true_k):\n",
    "    freq = frequencies_dict(i)\n",
    "    makeImage(freq)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2401977b-f350-419d-a6d1-cc5deb68c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for i in range(true_k):\n",
    "    freq = frequencies_dict(i)\n",
    "    makeImage(freq)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(true_k):\n",
    "    freq = frequencies_dict(i)\n",
    "    makeImage(freq)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(true_k):\n",
    "    freq = frequencies_dict(i)\n",
    "    makeImage(freq)\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(true_k):\n",
    "    freq = frequencies_dict(i)\n",
    "    makeImage(freq)\n",
    "    print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
